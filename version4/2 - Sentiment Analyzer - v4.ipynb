{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "RS = sum(list(map(ord, 'Dale Boca')))\n",
    "\n",
    "import pandas as pd\n",
    "pd.option_context('display.max_rows', None, 'display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "import numpy as np\n",
    "import os;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preproceso de datos y Cálculo de la Polaridad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('nflx_all_tweets.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35982</th>\n",
       "      <td>2017-11-23</td>\n",
       "      <td>Scandal creator  Why I left ABC for Netflix DIS NFLX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35957</th>\n",
       "      <td>2017-11-23</td>\n",
       "      <td>Netflix Inc NFLX Stock Live Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35956</th>\n",
       "      <td>2017-11-23</td>\n",
       "      <td>Netflix NFLX Market Valuation Rose While Parkwood Trimmed by  Million Its Holding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35955</th>\n",
       "      <td>2017-11-23</td>\n",
       "      <td>Strong Price Compression from a Daily Rising Wedge   red  NFLX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35954</th>\n",
       "      <td>2017-11-23</td>\n",
       "      <td>VMware  Analysts think these stocks can be a game changer Netflix Inc NASDAQ NFLX Retail        via TheCloudNetwork  on</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             date  \\\n",
       "35982  2017-11-23   \n",
       "35957  2017-11-23   \n",
       "35956  2017-11-23   \n",
       "35955  2017-11-23   \n",
       "35954  2017-11-23   \n",
       "\n",
       "                                                                                                                           text  \n",
       "35982  Scandal creator  Why I left ABC for Netflix DIS NFLX                                                                      \n",
       "35957  Netflix Inc NFLX Stock Live Analysis                                                                                      \n",
       "35956  Netflix NFLX Market Valuation Rose While Parkwood Trimmed by  Million Its Holding                                         \n",
       "35955     Strong Price Compression from a Daily Rising Wedge   red  NFLX                                                         \n",
       "35954  VMware  Analysts think these stocks can be a game changer Netflix Inc NASDAQ NFLX Retail        via TheCloudNetwork  on   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['date'] = pd.to_datetime(tweets.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "#from nltk.stem.snowball import SnowballStemmer\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#from sklearn.feature_extraction import text\n",
    "#from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "#extra = []\n",
    "#extra = [e.lower() for e in extra]\n",
    "#stop_words = text.ENGLISH_STOP_WORDS.union(extra)\n",
    "\n",
    "# stemming resuelve terminaciones y derivaciones de palabras, pero sin una base morfológica\n",
    "#stemmer    = SnowballStemmer(\"english\") \n",
    "# resuelve en base a un diccionario morfológico de las palabras\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#countv = CountVectorizer(ngram_range = (1,3), max_df=.20, min_df=0.01, stop_words=stop_words, lowercase=True)\n",
    "#tfid   = TfidfTransformer(norm='l2')\n",
    "\n",
    "\n",
    "def preprocessing_text(texto):\n",
    "    words = word_tokenize(texto)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/arturo.torrestey/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicializo el SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader_sent_analyzer(texto):\n",
    "    # calculo el compound ratio promedio de la sentences que se encuentran en texto\n",
    "    # texto viene como una lista de palabras\n",
    "    pos_val = []\n",
    "    neg_val = []\n",
    "    neu_val = []\n",
    "    for sent in texto:\n",
    "        ss = sid.polarity_scores(sent)\n",
    "        pos_val.append(ss['pos'])\n",
    "        neg_val.append(ss['neg'])\n",
    "        neu_val.append(ss['neu'])\n",
    "        \n",
    "    return (np.mean(pos_val), np.mean(neg_val), np.mean(neu_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculo de polaridad\n",
    "        \n",
    "# preproceso los datos de la columna texto y defino el vader compound ratio\n",
    "tweets['tuplas'] = [vader_sent_analyzer(preprocessing_text(t)) for t in tweets.text]\n",
    "tweets['pos'] = tweets['tuplas'].apply(lambda x: x[0])\n",
    "tweets['neg'] = tweets['tuplas'].apply(lambda x: x[1])\n",
    "tweets['neu'] = tweets['tuplas'].apply(lambda x: x[2])\n",
    "\n",
    "# selecciono las columnas para trabajar\n",
    "tweets = tweets.loc[:, ['date', 'pos', 'neg', 'neu']]\n",
    "\n",
    "# agrupo tweets por fecha, promediando los scores de pos, neg y neutros\n",
    "grouped_tweets = tweets.groupby('date').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo de Retornos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# armo el nombre del archivo a leer de los parametros\n",
    "dir_name = '../data/original'\n",
    "stock_name = 'NFLX_all_prices.txt'\n",
    "index_name = 'NDX_all_prices.txt'\n",
    "\n",
    "stockname  = os.path.join(dir_name, stock_name)\n",
    "indexname  = os.path.join(dir_name, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leo los archivos y le cambio los nombres a las columnas\n",
    "stk = pd.read_csv(stockname, usecols=['Date', 'Adj Close'])\n",
    "ind = pd.read_csv(indexname,usecols=['Date', 'Adj Close'])\n",
    "stk.columns = ['date', 'nflx']\n",
    "ind.columns = ['date', 'ndx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mergeo los df con los precios de la acción y el índice\n",
    "data = pd.merge(stk, ind, on='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paso la fecha a formato datetime\n",
    "data['date'] = pd.to_datetime(data.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# armo un df con el total de los días en el rango de fechas de los archivos provistos\n",
    "dias_totales = pd.DataFrame(pd.date_range(data.date.min(), data.date.max(), freq='D'), columns=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mergeo para buscar los días sin valor en precios (fin de semana y feriados)\n",
    "data = pd.merge(dias_totales, data, on='date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolo los nans con función cuadrática\n",
    "data = data.interpolate(method='quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculo el retorno logarítmico\n",
    "data['log_ret nflx'] = np.log(data.nflx / data.nflx.shift(1))\n",
    "data['log_ret ndx']  = np.log(data.ndx / data.ndx.shift(1))\n",
    "data['exc_ret']      = (data['log_ret nflx'] - data['log_ret ndx']) > 0\n",
    "data['exc_ret%']     = data['log_ret nflx'] - data['log_ret ndx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# armo el df para retornar con la fecha y el excess return de la acción eliminando\n",
    "# la primera observación (sin retorno calculado)\n",
    "grouped_returns = data.loc[1: , ['date', 'exc_ret', 'exc_ret%']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>exc_ret</th>\n",
       "      <th>exc_ret%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>True</td>\n",
       "      <td>0.018367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-11-29</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.039507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-11-30</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.011582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-12-02</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.000404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  exc_ret  exc_ret%\n",
       "1 2017-11-28  True     0.018367\n",
       "2 2017-11-29  False   -0.039507\n",
       "3 2017-11-30  False   -0.011582\n",
       "4 2017-12-01  True     0.000300\n",
       "5 2017-12-02  False   -0.000404"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_returns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merger de los Tweets y Retornos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.merge(grouped_tweets, grouped_returns, on='date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna(subset=['exc_ret%'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>exc_ret</th>\n",
       "      <th>exc_ret%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>0.040453</td>\n",
       "      <td>0.010565</td>\n",
       "      <td>0.930767</td>\n",
       "      <td>True</td>\n",
       "      <td>0.018367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017-11-29</td>\n",
       "      <td>0.037469</td>\n",
       "      <td>0.014695</td>\n",
       "      <td>0.925284</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.039507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017-11-30</td>\n",
       "      <td>0.051480</td>\n",
       "      <td>0.021632</td>\n",
       "      <td>0.913808</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.011582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>0.053825</td>\n",
       "      <td>0.016789</td>\n",
       "      <td>0.911705</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017-12-02</td>\n",
       "      <td>0.041612</td>\n",
       "      <td>0.019572</td>\n",
       "      <td>0.909338</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.000404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date       pos       neg       neu exc_ret  exc_ret%\n",
       "5 2017-11-28  0.040453  0.010565  0.930767  True    0.018367\n",
       "6 2017-11-29  0.037469  0.014695  0.925284  False  -0.039507\n",
       "7 2017-11-30  0.051480  0.021632  0.913808  False  -0.011582\n",
       "8 2017-12-01  0.053825  0.016789  0.911705  True    0.000300\n",
       "9 2017-12-02  0.041612  0.019572  0.909338  False  -0.000404"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('dataset_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
